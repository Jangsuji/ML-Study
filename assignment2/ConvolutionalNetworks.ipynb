{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ConvolutionalNetworks.ipynb","provenance":[],"authorship_tag":"ABX9TyPLPZBB9xgmbHgWx8SNfdsa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jJfZphPBFYwk"},"outputs":[],"source":["def conv_forward_naive(x, w, b, conv_param):\n","    \"\"\"A naive implementation of the forward pass for a convolutional layer.\n","\n","    The input consists of N data points, each with C channels, height H and\n","    width W. We convolve each input with F different filters, where each filter\n","    spans all C channels and has height HH and width WW.\n","\n","    Input:\n","    - x: Input data of shape (N, C, H, W)\n","    - w: Filter weights of shape (F, C, HH, WW)\n","    - b: Biases, of shape (F,)\n","    - conv_param: A dictionary with the following keys:\n","      - 'stride': The number of pixels between adjacent receptive fields in the\n","        horizontal and vertical directions.\n","      - 'pad': The number of pixels that will be used to zero-pad the input.\n","\n","    During padding, 'pad' zeros should be placed symmetrically (i.e equally on both sides)\n","    along the height and width axes of the input. Be careful not to modfiy the original\n","    input x directly.\n","\n","    Returns a tuple of:\n","    - out: Output data, of shape (N, F, H', W') where H' and W' are given by\n","      H' = 1 + (H + 2 * pad - HH) / stride\n","      W' = 1 + (W + 2 * pad - WW) / stride\n","    - cache: (x, w, b, conv_param)\n","    \"\"\"\n","    out = None\n","    ###########################################################################\n","    # TODO: Implement the convolutional forward pass.                         #\n","    # Hint: you can use the function np.pad for padding.                      #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    N, C, H, W = x.shape\n","    F, _, HH, WW = w.shape\n","    stride, pad = conv_param['stride'], conv_param['pad']\n","    H_out = 1 + (H + 2 * pad - HH) // stride  # Use `//` for python3\n","    W_out = 1 + (W + 2 * pad - WW) // stride\n","    out = np.zeros((N, F, H_out, W_out))\n","\n","    x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n","\n","    for n in range(N):\n","    \tfor f in range(F):\n","    \t\tfor h_out in range(H_out):\n","    \t\t\tfor w_out in range(W_out):\n","    \t\t\t\tout[n, f, h_out, w_out] = np.sum(\n","    \t\t\t\t\tx_pad[n, :, h_out*stride:h_out*stride+HH, w_out*stride:w_out*stride+WW]*w[f, :]) + b[f]\n","\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = (x, w, b, conv_param)\n","    return out, cache"]},{"cell_type":"code","source":["def conv_backward_naive(dout, cache):\n","    \"\"\"A naive implementation of the backward pass for a convolutional layer.\n","\n","    Inputs:\n","    - dout: Upstream derivatives.\n","    - cache: A tuple of (x, w, b, conv_param) as in conv_forward_naive\n","\n","    Returns a tuple of:\n","    - dx: Gradient with respect to x\n","    - dw: Gradient with respect to w\n","    - db: Gradient with respect to b\n","    \"\"\"\n","    dx, dw, db = None, None, None\n","    ###########################################################################\n","    # TODO: Implement the convolutional backward pass.                        #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    \n","    x, w, b, conv_param = cache\n","    N, C, H, W = x.shape\n","    F, _, HH, WW = w.shape\n","    _, _, H_out, W_out = dout.shape\n","    stride, pad = conv_param['stride'], conv_param['pad']\n","\n","    x_pad = np.pad(x, ((0,), (0,), (pad,), (pad,)), mode='constant', constant_values=0)\n","\n","    dx_pad = np.zeros_like(x_pad)\n","    dw = np.zeros_like(w)\n","    db = np.zeros_like(b)\n","\n","    # Ref: https://github.com/MahanFathi/CS231/blob/ecab92ed8627ea0ea513a54fc2019516d446c106/assignment2/cs231n/layers.py#L483-L491\n","    for n in range(N):\n","    \tfor f in range(F):\n","    \t\tdb[f] += np.sum(dout[n, f])\n","    \t\tfor h_out in range(H_out):\n","    \t\t\tfor w_out in range(W_out):\n","    \t\t\t\tdw[f] += x_pad[n, :, h_out*stride:h_out*stride+HH, w_out*stride:w_out*stride+WW] * \\\n","    \t\t\t\tdout[n, f, h_out, w_out]\n","    \t\t\t\tdx_pad[n, :, h_out*stride:h_out*stride+HH, w_out*stride:w_out*stride+WW] += w[f] * \\\n","    \t\t\t\tdout[n, f, h_out, w_out]\n","\n","    dx = dx_pad[:, :, pad:pad+H, pad:pad+W]\n","\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dx, dw, db"],"metadata":{"id":"aNa-QTGkMBeN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def max_pool_forward_naive(x, pool_param):\n","    \"\"\"A naive implementation of the forward pass for a max-pooling layer.\n","\n","    Inputs:\n","    - x: Input data, of shape (N, C, H, W)\n","    - pool_param: dictionary with the following keys:\n","      - 'pool_height': The height of each pooling region\n","      - 'pool_width': The width of each pooling region\n","      - 'stride': The distance between adjacent pooling regions\n","\n","    No padding is necessary here, eg you can assume:\n","      - (H - pool_height) % stride == 0\n","      - (W - pool_width) % stride == 0\n","\n","    Returns a tuple of:\n","    - out: Output data, of shape (N, C, H', W') where H' and W' are given by\n","      H' = 1 + (H - pool_height) / stride\n","      W' = 1 + (W - pool_width) / stride\n","    - cache: (x, pool_param)\n","    \"\"\"\n","    out = None\n","    ###########################################################################\n","    # TODO: Implement the max-pooling forward pass                            #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    \n","    N, C, H, W = x.shape\n","    pool_height = pool_param['pool_height']\n","    pool_width = pool_param['pool_width']\n","    stride = pool_param['stride']\n","    H_out = 1 + (H - pool_height) // stride\n","    W_out = 1 + (W - pool_width) // stride\n","    out = np.zeros((N, C, H_out, W_out))\n","\n","    for n in range(N):\n","    \tfor h_out in range(H_out):\n","    \t\tfor w_out in range(W_out):\n","    \t\t\tout[n, :, h_out, w_out] = np.max(x[n, :, h_out*stride:h_out*stride+pool_height,\n","    \t\t\t\tw_out*stride:w_out*stride+pool_width], axis=(-1, -2)) # axis can also be (1, 2)\n","\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = (x, pool_param)\n","    return out, cache"],"metadata":{"id":"18fOQ3_fTKTK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def max_pool_backward_naive(dout, cache):\n","    \"\"\"A naive implementation of the backward pass for a max-pooling layer.\n","\n","    Inputs:\n","    - dout: Upstream derivatives\n","    - cache: A tuple of (x, pool_param) as in the forward pass.\n","\n","    Returns:\n","    - dx: Gradient with respect to x\n","    \"\"\"\n","    dx = None\n","    ###########################################################################\n","    # TODO: Implement the max-pooling backward pass                           #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    \n","    x, pool_param = cache\n","    N, C, H, W = x.shape\n","    pool_height = pool_param['pool_height']\n","    pool_width = pool_param['pool_width']\n","    stride = pool_param['stride']\n","    H_out = 1 + (H - pool_height) // stride\n","    W_out = 1 + (W - pool_width) // stride\n","    dx = np.zeros_like(x)\n","\n","    for n in range(N):\n","    \tfor c in range(C):\n","    \t\tfor h in range(H_out):\n","    \t\t\tfor w in range(W_out):\n","    \t\t\t\t# Find the index (row, col) of the max value\n","    \t\t\t\t# Ref: examples of https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.argmax.html\n","    \t\t\t\tind = np.unravel_index(np.argmax(x[n, c, h*stride:h*stride+pool_height,\n","    \t\t\t\t\tw*stride:w*stride+pool_width], axis=None), (pool_height, pool_width))\n","    \t\t\t\t\n","    \t\t\t\tdx[n, c, h*stride:h*stride+pool_height, w*stride:w*stride+pool_width][ind] = \\\n","    \t\t\t\tdout[n, c, h, w]\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dx"],"metadata":{"id":"Jnf2z0StTNEd"},"execution_count":null,"outputs":[]}]}