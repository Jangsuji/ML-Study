{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FullyConnectedNets.ipynb","provenance":[],"authorship_tag":"ABX9TyPrA6GQm6/vmoB1bsiHYpfi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Vanilla update**\n","> x += - learning_rate * dx  "],"metadata":{"id":"nkkCOsqCduuF"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eeSUPDHOV_lL"},"outputs":[],"source":["def sgd_momentum(w, dw, config=None):\n","    \"\"\"\n","    Performs stochastic gradient descent with momentum.\n","\n","    config format:\n","    - learning_rate: Scalar learning rate.\n","    - momentum: Scalar between 0 and 1 giving the momentum value.\n","      Setting momentum = 0 reduces to sgd.\n","    - velocity: A numpy array of the same shape as w and dw used to store a\n","      moving average of the gradients.\n","    \"\"\"\n","    if config is None:\n","        config = {}\n","    config.setdefault(\"learning_rate\", 1e-2)\n","    config.setdefault(\"momentum\", 0.9)\n","    v = config.get(\"velocity\", np.zeros_like(w))\n","\n","    next_w = None\n","    ###########################################################################\n","    # TODO: Implement the momentum update formula. Store the updated value in #\n","    # the next_w variable. You should also use and update the velocity v.     #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    v = config['momentum'] * v - config['learning_rate'] * dw\n","    next_w = w + v\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    config[\"velocity\"] = v\n","\n","    return next_w, config"]},{"cell_type":"markdown","source":["**Momentum update**\n","> v = mu * v - learning_rate * dx  \n","x += v  "],"metadata":{"id":"RdgT0MkMWbg0"}},{"cell_type":"code","source":["def rmsprop(w, dw, config=None):\n","    \"\"\"\n","    Uses the RMSProp update rule, which uses a moving average of squared\n","    gradient values to set adaptive per-parameter learning rates.\n","\n","    config format:\n","    - learning_rate: Scalar learning rate.\n","    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared\n","      gradient cache.\n","    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n","    - cache: Moving average of second moments of gradients.\n","    \"\"\"\n","    if config is None:\n","        config = {}\n","    config.setdefault(\"learning_rate\", 1e-2)\n","    config.setdefault(\"decay_rate\", 0.99)\n","    config.setdefault(\"epsilon\", 1e-8)\n","    config.setdefault(\"cache\", np.zeros_like(w))\n","\n","    next_w = None\n","    ###########################################################################\n","    # TODO: Implement the RMSprop update formula, storing the next value of w #\n","    # in the next_w variable. Don't forget to update cache value stored in    #\n","    # config['cache'].                                                        #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    config['cache'] = config['decay_rate'] * config['cache'] + (1 - config['decay_rate']) * dw**2\n","    next_w = w - config['learning_rate'] * dw / (np.sqrt(config['cache']) + config['epsilon'])\n","\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","\n","    return next_w, config"],"metadata":{"id":"ZKpKIibpWrGk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**RMSprop**\n",">cache = decay_rate * cache + (1 - decay_rate) * dx**2  \n","x += - learning_rate * dx / (np.sqrt(cache) + eps)"],"metadata":{"id":"oQH__p3KYjt4"}},{"cell_type":"code","source":["def adam(w, dw, config=None):\n","    \"\"\"\n","    Uses the Adam update rule, which incorporates moving averages of both the\n","    gradient and its square and a bias correction term.\n","\n","    config format:\n","    - learning_rate: Scalar learning rate.\n","    - beta1: Decay rate for moving average of first moment of gradient.\n","    - beta2: Decay rate for moving average of second moment of gradient.\n","    - epsilon: Small scalar used for smoothing to avoid dividing by zero.\n","    - m: Moving average of gradient.\n","    - v: Moving average of squared gradient.\n","    - t: Iteration number.\n","    \"\"\"\n","    if config is None:\n","        config = {}\n","    config.setdefault(\"learning_rate\", 1e-3)\n","    config.setdefault(\"beta1\", 0.9)\n","    config.setdefault(\"beta2\", 0.999)\n","    config.setdefault(\"epsilon\", 1e-8)\n","    config.setdefault(\"m\", np.zeros_like(w))\n","    config.setdefault(\"v\", np.zeros_like(w))\n","    config.setdefault(\"t\", 0)\n","\n","    next_w = None\n","    ###########################################################################\n","    # TODO: Implement the Adam update formula, storing the next value of w in #\n","    # the next_w variable. Don't forget to update the m, v, and t variables   #\n","    # stored in config.                                                       #\n","    #                                                                         #\n","    # NOTE: In order to match the reference output, please modify t _before_  #\n","    # using it in any calculations.                                           #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    # Update t\n","    config['t'] += 1\n","\n","    # Ref: http://cs231n.github.io/neural-networks-3/#ada\n","    config['m'] = config['beta1'] * config['m'] + (1 - config['beta1']) * dw\n","    mt = config['m'] / (1 - config['beta1']**config['t'])\n","    config['v'] = config['beta2'] * config['v'] + (1 - config['beta2']) * dw**2\n","    vt = config['v'] / (1 - config['beta2']**config['t'])\n","    next_w = w - config['learning_rate'] * mt / (np.sqrt(vt) + config['epsilon'])\n","\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","\n","    return next_w, config"],"metadata":{"id":"1cBqWtJLYqvb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Adam**\n",">m = beta1 * m + (1-beta1) * dx  \n","v = beta2 * v + (1-beta2) * (dx**2)  \n","x += - learning_rate * m / (np.sqrt(v) + eps)"],"metadata":{"id":"MxYd59PtYzmi"}},{"cell_type":"code","source":[""],"metadata":{"id":"a4DbywAvY4BW"},"execution_count":null,"outputs":[]}]}