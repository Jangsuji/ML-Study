{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM.ipynb","provenance":[],"authorship_tag":"ABX9TyMy3Qoer0lgvmqM49tZ+uL1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multiclass Support Vector Machine exercise"],"metadata":{"id":"w90FErJNMLP3"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"Bx3wGP6KKgyh","executionInfo":{"status":"ok","timestamp":1646722478693,"user_tz":-540,"elapsed":461,"user":{"displayName":"장수지","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06919900695778449164"}}},"outputs":[],"source":["def svm_loss_naive(W, X, y, reg):\n","    \"\"\"\n","    Structured SVM loss function, naive implementation (with loops).\n","\n","    Inputs have dimension D, there are C classes, and we operate on minibatches\n","    of N examples.\n","\n","    Inputs:\n","    - W: A numpy array of shape (D, C) containing weights.\n","    - X: A numpy array of shape (N, D) containing a minibatch of data.\n","    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","      that X[i] has label c, where 0 <= c < C.\n","    - reg: (float) regularization strength\n","\n","    Returns a tuple of:\n","    - loss as single float\n","    - gradient with respect to weights W; an array of same shape as W\n","    \"\"\"\n","    dW = np.zeros(W.shape)  # initialize the gradient as zero\n","\n","    # compute the loss and the gradient\n","    num_classes = W.shape[1]\n","    num_train = X.shape[0]\n","    loss = 0.0\n","    for i in range(num_train):\n","        scores = X[i].dot(W)\n","        correct_class_score = scores[y[i]]\n","        for j in range(num_classes):\n","            if j == y[i]:\n","                continue\n","            margin = scores[j] - correct_class_score + 1  # note delta = 1\n","            if margin > 0:\n","                loss += margin\n","                dW[:, y[i]] -= X[i]\n","                dW[:, j] += X[i]\n","\n","    # Right now the loss is a sum over all training examples, but we want it\n","    # to be an average instead so we divide by num_train.\n","    loss /= num_train\n","    dW /= num_train\n","\n","    # Add regularization to the loss.\n","    loss += reg * np.sum(W * W)\n","    dW += 2*reg*W\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Compute the gradient of the loss function and store it dW.                #\n","    # Rather that first computing the loss and then computing the derivative,   #\n","    # it may be simpler to compute the derivative at the same time that the     #\n","    # loss is being computed. As a result you may need to modify some of the    #\n","    # code above to compute the gradient.                                       #\n","    #############################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    return loss, dW"]},{"cell_type":"markdown","source":["**SVM loss**\n","\n","$L_{i} = \\sum_{j\\ne y_{i}}max(0,s_{j}-s{y_{i}}+Δ)$  \n","$L_{i} = \\sum_{j\\ne y_{i}}[max(0,w^T_{j}x_{i}-w^T_{y_i}x_{i}+Δ)]$\n","\n","**SVM gradient**\n","\n","$ \\nabla_{w_{y_{i}}} L_{i} = -(\\sum_{j\\ne y_{i}}1(w^T_{j}x_{i}-w^T_{y_i}x_{i}+Δ>0))x_{i}$  \n","$ \\nabla_{w_{j}} L_{i} = 1(w^T_{j}x_{i}-w^T_{y_i}x_{i}+Δ>0)x_{i}$\n","\n","**SVM update**\n","\n","$ \\nabla_{w_{y_{i}}} L_{i} = \\nabla_{w_{y_{i}}} L_{i} - x_{i}$  \n","$ \\nabla_{w_{j}} L_{i} = \\nabla_{w_{j}} L_{i} + x_{i}$"],"metadata":{"id":"9dI0EVXiMKS_"}},{"cell_type":"code","source":["def svm_loss_vectorized(W, X, y, reg):\n","    \"\"\"\n","    Structured SVM loss function, vectorized implementation.\n","\n","    Inputs and outputs are the same as svm_loss_naive.\n","    \"\"\"\n","    loss = 0.0\n","    dW = np.zeros(W.shape)  # initialize the gradient as zero\n","    num_train = X.shape[0]\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Implement a vectorized version of the structured SVM loss, storing the    #\n","    # result in loss.                                                           #\n","    #############################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    scores = X.dot(W)\n","    correct_class_score = scores[np.arange(num_train), y].reshape(-1, 1)\n"," \n","    margin = np.maximum(0,scores - correct_class_score + 1)\n","    margin[np.arange(num_train), y ] =0\n","    \n","    loss = (np.sum(margin) / num_train) + reg*np.sum(W*W)\n","\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Implement a vectorized version of the gradient for the structured SVM     #\n","    # loss, storing the result in dW.                                           #\n","    #                                                                           #\n","    # Hint: Instead of computing the gradient from scratch, it may be easier    #\n","    # to reuse some of the intermediate values that you used to compute the     #\n","    # loss.                                                                     #\n","    #############################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    mask = np.where(margin>0,1,0)\n","    mask[np.arange(num_train), y] = -np.sum(mask, axis = 1)\n","    dW += np.dot(X.T, mask) / num_train\n","    \n","    dW += W*2*reg\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    return loss, dW"],"metadata":{"id":"MzFOyL1-Kjrv","executionInfo":{"status":"ok","timestamp":1646722479163,"user_tz":-540,"elapsed":1,"user":{"displayName":"장수지","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06919900695778449164"}}},"execution_count":3,"outputs":[]}]}