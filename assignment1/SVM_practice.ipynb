{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM_practice.ipynb","provenance":[],"mount_file_id":"1OmBBc4YSx1ZbYE1YN2o562B4j7yPiAWG","authorship_tag":"ABX9TyPnY6YIBR37yVmq71vb1SoH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multiclass Support Vector Machine exercise"],"metadata":{"id":"w90FErJNMLP3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bx3wGP6KKgyh"},"outputs":[],"source":["def svm_loss_naive(W, X, y, reg):\n","    \"\"\"\n","    Structured SVM loss function, naive implementation (with loops).\n","\n","    Inputs have dimension D, there are C classes, and we operate on minibatches\n","    of N examples.\n","\n","    Inputs:\n","    - W: A numpy array of shape (D, C) containing weights.\n","    - X: A numpy array of shape (N, D) containing a minibatch of data.\n","    - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n","      that X[i] has label c, where 0 <= c < C.\n","    - reg: (float) regularization strength\n","\n","    Returns a tuple of:\n","    - loss as single float\n","    - gradient with respect to weights W; an array of same shape as W\n","    \"\"\"\n","    dW = np.zeros(W.shape)  # initialize the gradient as zero\n","\n","    # compute the loss and the gradient\n","    num_classes = W.shape[1]\n","    num_train = X.shape[0]\n","    loss = 0.0\n","    for i in range(num_train):\n","        scores = X[i].dot(W)\n","        correct_class_score = scores[y[i]]\n","        for j in range(num_classes):\n","            if j == y[i]:\n","                continue\n","            margin = scores[j] - correct_class_score + 1  # note delta = 1\n","            if margin > 0:\n","                loss += margin\n","                dW[:, y[i]] -= X[i]\n","                dW[:, j] += X[i]\n","\n","    # Right now the loss is a sum over all training examples, but we want it\n","    # to be an average instead so we divide by num_train.\n","    loss /= num_train\n","    dW /= num_train\n","\n","    # Add regularization to the loss.\n","    loss += reg * np.sum(W * W)\n","    dW += 2*reg*W\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Compute the gradient of the loss function and store it dW.                #\n","    # Rather that first computing the loss and then computing the derivative,   #\n","    # it may be simpler to compute the derivative at the same time that the     #\n","    # loss is being computed. As a result you may need to modify some of the    #\n","    # code above to compute the gradient.                                       #\n","    #############################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    return loss, dW"]},{"cell_type":"markdown","source":["**SVM loss**\n","\n",">$L_{i} = \\sum_{j\\ne y_{i}}max(0,s_{j}-s_{y_{i}}+Δ)$  \n","$L_{i} = \\sum_{j\\ne y_{i}}max(0,w^T_{j}x_{i}-w^T_{y_i}x_{i}+Δ)$\n","\n","**SVM gradient**\n","\n",">$ \\nabla_{w_{y_{i}}} L_{i} = -(\\sum_{j\\ne y_{i}}1(w^T_{j}x_{i}-w^T_{y_i}x_{i}+Δ>0))x_{i}$  \n","$ \\nabla_{w_{j}} L_{i} = 1(w^T_{j}x_{i}-w^T_{y_i}x_{i}+Δ>0)x_{i}$\n"],"metadata":{"id":"9dI0EVXiMKS_"}},{"cell_type":"markdown","source":["<img src=\"note/svm.PNG\" width=\"1200px\" height=\"500px\" title=\"svm\"/>"],"metadata":{"id":"_oNEQq4oUtJN"}},{"cell_type":"code","source":["def svm_loss_vectorized(W, X, y, reg):\n","    \"\"\"\n","    Structured SVM loss function, vectorized implementation.\n","\n","    Inputs and outputs are the same as svm_loss_naive.\n","    \"\"\"\n","    loss = 0.0\n","    dW = np.zeros(W.shape)  # initialize the gradient as zero\n","    num_train = X.shape[0]\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Implement a vectorized version of the structured SVM loss, storing the    #\n","    # result in loss.                                                           #\n","    #############################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    scores = X.dot(W)\n","    correct_class_score = scores[np.arange(num_train), y].reshape(-1, 1)\n"," \n","    margin = np.maximum(0,scores - correct_class_score + 1)\n","    margin[np.arange(num_train), y ] =0\n","    \n","    loss = (np.sum(margin) / num_train) + reg*np.sum(W*W)\n","\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    #############################################################################\n","    # TODO:                                                                     #\n","    # Implement a vectorized version of the gradient for the structured SVM     #\n","    # loss, storing the result in dW.                                           #\n","    #                                                                           #\n","    # Hint: Instead of computing the gradient from scratch, it may be easier    #\n","    # to reuse some of the intermediate values that you used to compute the     #\n","    # loss.                                                                     #\n","    #############################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    mask = np.where(margin>0,1,0)\n","    mask[np.arange(num_train), y] = -np.sum(mask, axis = 1)\n","    dW = np.dot(X.T, mask) / num_train\n","    \n","    dW += W*2*reg\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    return loss, dW"],"metadata":{"id":"MzFOyL1-Kjrv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"note/svm_vectorization.PNG\" title=\"svm_vec\"/>"],"metadata":{"id":"kLw31M7Ai_VX"}},{"cell_type":"markdown","source":["# Linear Classifier#"],"metadata":{"id":"yX7zabZeRTyT"}},{"cell_type":"code","source":["class LinearClassifier(object):\n","    def __init__(self):\n","        self.W = None\n","\n","    def train(\n","        self,\n","        X,\n","        y,\n","        learning_rate=1e-3,\n","        reg=1e-5,\n","        num_iters=100,\n","        batch_size=200,\n","        verbose=False,\n","    ):\n","        \"\"\"\n","        Train this linear classifier using stochastic gradient descent.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) containing training data; there are N\n","          training samples each of dimension D.\n","        - y: A numpy array of shape (N,) containing training labels; y[i] = c\n","          means that X[i] has label 0 <= c < C for C classes.\n","        - learning_rate: (float) learning rate for optimization.\n","        - reg: (float) regularization strength.\n","        - num_iters: (integer) number of steps to take when optimizing\n","        - batch_size: (integer) number of training examples to use at each step.\n","        - verbose: (boolean) If true, print progress during optimization.\n","\n","        Outputs:\n","        A list containing the value of the loss function at each training iteration.\n","        \"\"\"\n","        num_train, dim = X.shape\n","        num_classes = (\n","            np.max(y) + 1\n","        )  # assume y takes values 0...K-1 where K is number of classes\n","        if self.W is None:\n","            # lazily initialize W\n","            self.W = 0.001 * np.random.randn(dim, num_classes)\n","\n","        # Run stochastic gradient descent to optimize W\n","        loss_history = []\n","        for it in range(num_iters):\n","\n","            #########################################################################\n","            # TODO:                                                                 #\n","            # Sample batch_size elements from the training data and their           #\n","            # corresponding labels to use in this round of gradient descent.        #\n","            # Store the data in X_batch and their corresponding labels in           #\n","            # y_batch; after sampling X_batch should have shape (batch_size, dim)   #\n","            # and y_batch should have shape (batch_size,)                           #\n","            #                                                                       #\n","            # Hint: Use np.random.choice to generate indices. Sampling with         #\n","            # replacement is faster than sampling without replacement.              #\n","            #########################################################################\n","            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","            batch = np.random.choice(range(num_train),batch_size,replace=True)\n","\n","            X_batch = X[batch]\n","            y_batch = y[batch]\n","\n","            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","            # evaluate loss and gradient\n","            loss, grad = self.loss(X_batch, y_batch, reg)\n","            loss_history.append(loss)\n","\n","            # perform parameter update\n","            #########################################################################\n","            # TODO:                                                                 #\n","            # Update the weights using the gradient and the learning rate.          #\n","            #########################################################################\n","            # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","            self.W -= learning_rate*grad\n","\n","            # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","            if verbose and it % 100 == 0:\n","                print(\"iteration %d / %d: loss %f\" % (it, num_iters, loss))\n","\n","        return loss_history"],"metadata":{"id":"MvVMfEAYRaW0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Gradient Descent**\n","> $ w := w - α \\frac{∂loss}{∂ w} $  "],"metadata":{"id":"gCvECz4eRiAL"}},{"cell_type":"code","source":["    def predict(self, X):\n","        \"\"\"\n","        Use the trained weights of this linear classifier to predict labels for\n","        data points.\n","\n","        Inputs:\n","        - X: A numpy array of shape (N, D) containing training data; there are N\n","          training samples each of dimension D.\n","\n","        Returns:\n","        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n","          array of length N, and each element is an integer giving the predicted\n","          class.\n","        \"\"\"\n","        y_pred = np.zeros(X.shape[0])\n","        ###########################################################################\n","        # TODO:                                                                   #\n","        # Implement this method. Store the predicted labels in y_pred.            #\n","        ###########################################################################\n","        # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","        scores = X.dot(self.W)\n","        y_pred = np.argmax(scores,axis=1)\n","\n","        # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","        return y_pred"],"metadata":{"id":"63EqTOxqTPCw"},"execution_count":null,"outputs":[]}]}