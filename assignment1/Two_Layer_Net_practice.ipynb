{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Two_Layer_Net_practice.ipynb","provenance":[],"mount_file_id":"1UmBMGt2pu-tOKRbX3oGfR7CvQSL9ScW4","authorship_tag":"ABX9TyNTiMEvwqgO+d6hua6S2p6E"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fully-Connected Neural Nets"],"metadata":{"id":"UoBqyFOZ6SgM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjPvAkDm54Ib"},"outputs":[],"source":["def affine_forward(x, w, b):\n","    \"\"\"\n","    Computes the forward pass for an affine (fully-connected) layer.\n","\n","    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n","    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n","    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n","    then transform it to an output vector of dimension M.\n","\n","    Inputs:\n","    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n","    - w: A numpy array of weights, of shape (D, M)\n","    - b: A numpy array of biases, of shape (M,)\n","\n","    Returns a tuple of:\n","    - out: output, of shape (N, M)\n","    - cache: (x, w, b)\n","    \"\"\"\n","    out = None\n","    ###########################################################################\n","    # TODO: Implement the affine forward pass. Store the result in out. You   #\n","    # will need to reshape the input into rows.                               #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    fixed_x = x.reshape(x.shape[0],-1)\n","    out = fixed_x.dot(w)+b\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = (x, w, b)\n","    return out, cache"]},{"cell_type":"markdown","source":["**Forward Propagation**\n","\n",">$ out = wx+b $  "],"metadata":{"id":"RWcNO60e6H8T"}},{"cell_type":"code","source":["def affine_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for an affine layer.\n","\n","    Inputs:\n","    - dout: Upstream derivative, of shape (N, M)\n","    - cache: Tuple of:\n","      - x: Input data, of shape (N, d_1, ... d_k)\n","      - w: Weights, of shape (D, M)\n","      - b: Biases, of shape (M,)\n","\n","    Returns a tuple of:\n","    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n","    - dw: Gradient with respect to w, of shape (D, M)\n","    - db: Gradient with respect to b, of shape (M,)\n","    \"\"\"\n","    x, w, b = cache\n","    dx, dw, db = None, None, None\n","    ###########################################################################\n","    # TODO: Implement the affine backward pass.                               #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    dx = dout.dot(w.T)\n","    dx = dx.reshape(-1,x.shape[1],x.shape[2])\n","    fixed_x = x.reshape(x.shape[0],-1)\n","    dw = fixed_x.T.dot(dout)\n","    db = np.sum(dout, axis=0)\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dx, dw, db"],"metadata":{"id":"mtazzi1c64GB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Back Propagation**\n","\n",">$ out = wx+b $  \n",">\n",">$ \\frac{dout}{dx} = w 　\\blacktriangleright 　 \\frac{dloss}{dx} = \\frac{dloss}{dout} ⋅ \\frac{dout}{dx} = \\frac{dloss}{dout} ⋅ w $   \n",">\n",">$ \\frac{dout}{dw} = x 　\\blacktriangleright 　 \\frac{dloss}{dw} = \\frac{dloss}{dout} ⋅ \\frac{dout}{dw} = \\frac{dloss}{dout} ⋅ x $   \n",">\n",">$ \\frac{dout}{db} = 1　\\blacktriangleright 　 \\frac{dloss}{db} = \\frac{dloss}{dout} ⋅ \\frac{dout}{db} = \\frac{dloss}{dout} $ "],"metadata":{"id":"V7sWbaKG6_sD"}},{"cell_type":"code","source":["def relu_forward(x):\n","    \"\"\"\n","    Computes the forward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - x: Inputs, of any shape\n","\n","    Returns a tuple of:\n","    - out: Output, of the same shape as x\n","    - cache: x\n","    \"\"\"\n","    out = None\n","    ###########################################################################\n","    # TODO: Implement the ReLU forward pass.                                  #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    out = np.maximum(0,x)\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = x\n","    return out, cache"],"metadata":{"id":"NOYzb-hZ8Xrt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"note/ReLU.png\" width=\"500px\" height=\"300px\" title=\"ReLU\" />\n","\n","\n","**ReLU Forward Propagation**\n","\n",">$ out =\n","\\begin{cases}\n","0, & x < 0 \\\\\n","x, & x \\ge 0\n","\\end{cases} $"],"metadata":{"id":"WoW4wHnl9NeI"}},{"cell_type":"code","source":["def relu_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - dout: Upstream derivatives, of any shape\n","    - cache: Input x, of same shape as dout\n","\n","    Returns:\n","    - dx: Gradient with respect to x\n","    \"\"\"\n","    dx, x = None, cache\n","    ###########################################################################\n","    # TODO: Implement the ReLU backward pass.                                 #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    mask = np.where(x>0,1,0)\n","    dx = dout*mask\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dx"],"metadata":{"id":"gvfdu3-Yx838","executionInfo":{"status":"ok","timestamp":1647220256063,"user_tz":-540,"elapsed":358,"user":{"displayName":"장수지","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06919900695778449164"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["**ReLU Back Propagation**\n","\n",">$ out =\n","\\begin{cases}\n","0, & x < 0 \\\\\n","x, & x \\ge 0\n","\\end{cases} $  \n",">\n",">$ \\frac{dout}{dx} =\n","\\begin{cases}\n","0, & x < 0 \\\\\n","1, & x \\ge 0\n","\\end{cases} $  \n","> 　$\\blacktriangledown$  \n",">$\\frac{dloss}{dx} = \\frac{dloss}{dout}⋅\\frac{dout}{dx}$\n",">$ \\frac{dloss}{dx} =\n","\\begin{cases}\n","0, & x < 0 \\\\\n","\\frac{dloss}{dout}, & x \\ge 0\n","\\end{cases} $ "],"metadata":{"id":"ujHsmYHR3TG7"}},{"cell_type":"code","source":["def svm_loss(x, y):\n","    \"\"\"\n","    Computes the loss and gradient using for multiclass SVM classification.\n","\n","    Inputs:\n","    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth\n","      class for the ith input.\n","    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n","      0 <= y[i] < C\n","\n","    Returns a tuple of:\n","    - loss: Scalar giving the loss\n","    - dx: Gradient of the loss with respect to x\n","    \"\"\"\n","    loss, dx = None, None\n","    ###########################################################################\n","    # TODO: Implement loss and gradient for multiclass SVM classification.    #\n","    # This will be similar to the svm loss vectorized implementation in       #\n","    # cs231n/classifiers/linear_svm.py.                                       #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    N = x.shape[0]\n","    correct_class_score = x[range(N), y].reshape(-1, 1)\n","    margin = np.maximum(0,x - correct_class_score + 1)\n","    margin[np.arange(N), y ] =0\n","    loss = np.sum(margin) / N\n","\n","    dx = np.zeros_like(x)\n","    dx[margin>0] = 1\n","    num_pos = np.sum(margin>0,axis=1)\n","    dx[range(N),y] -= num_pos\n","    dx /= N\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return loss, dx"],"metadata":{"id":"VJFTQICS4Xrg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**SVM Back Propagation**\n","\n",">$ margin = w^T_{j}x_{i}-w^T_{y_i}x_{i}+1 $\n",">  \n",">$ L_{i} = \\sum_{j\\ne y_{i}}max(0,w^T_{j}x_{i}-w^T_{y_i}x_{i}+1) $  \n",">\n",">$ \\frac{dL_{i}}{dw^T_{j}x_{i}} = \\begin{cases}\n","0, & margin \\le 0 \\\\\n","1, & margin > 0\n","\\end{cases} $  \n",">\n",">$ \\frac{dL_{i}}{dw^T_{y_{i}}x_{i}} = \\begin{cases}\n","0, & margin \\le 0 \\\\\n","-1, & margin > 0\n","\\end{cases} $  \n",">"],"metadata":{"id":"hW5FvC5dZZXw"}},{"cell_type":"code","source":[""],"metadata":{"id":"6Rmp-eDOabf-"},"execution_count":null,"outputs":[]}]}