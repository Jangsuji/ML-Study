{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Two_Layer_Net_practice.ipynb","provenance":[],"mount_file_id":"1UmBMGt2pu-tOKRbX3oGfR7CvQSL9ScW4","authorship_tag":"ABX9TyMl245lXl9woKRQ/HDrUE6s"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Fully-Connected Neural Nets"],"metadata":{"id":"UoBqyFOZ6SgM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fjPvAkDm54Ib"},"outputs":[],"source":["def affine_forward(x, w, b):\n","    \"\"\"\n","    Computes the forward pass for an affine (fully-connected) layer.\n","\n","    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n","    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n","    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n","    then transform it to an output vector of dimension M.\n","\n","    Inputs:\n","    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n","    - w: A numpy array of weights, of shape (D, M)\n","    - b: A numpy array of biases, of shape (M,)\n","\n","    Returns a tuple of:\n","    - out: output, of shape (N, M)\n","    - cache: (x, w, b)\n","    \"\"\"\n","    out = None\n","    ###########################################################################\n","    # TODO: Implement the affine forward pass. Store the result in out. You   #\n","    # will need to reshape the input into rows.                               #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    fixed_x = x.reshape(x.shape[0],-1)\n","    out = fixed_x.dot(w)+b\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = (x, w, b)\n","    return out, cache"]},{"cell_type":"markdown","source":["**Forward Propagation**\n","\n",">$ out = wx+b $  "],"metadata":{"id":"RWcNO60e6H8T"}},{"cell_type":"code","source":["def affine_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for an affine layer.\n","\n","    Inputs:\n","    - dout: Upstream derivative, of shape (N, M)\n","    - cache: Tuple of:\n","      - x: Input data, of shape (N, d_1, ... d_k)\n","      - w: Weights, of shape (D, M)\n","      - b: Biases, of shape (M,)\n","\n","    Returns a tuple of:\n","    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n","    - dw: Gradient with respect to w, of shape (D, M)\n","    - db: Gradient with respect to b, of shape (M,)\n","    \"\"\"\n","    x, w, b = cache\n","    dx, dw, db = None, None, None\n","    ###########################################################################\n","    # TODO: Implement the affine backward pass.                               #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    dx = dout.dot(w.T)\n","    dx = dx.reshape(-1,x.shape[1],x.shape[2])\n","    fixed_x = x.reshape(x.shape[0],-1)\n","    dw = fixed_x.T.dot(dout)\n","    db = np.sum(dout, axis=0)\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dx, dw, db"],"metadata":{"id":"mtazzi1c64GB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Back Propagation**\n","\n",">$ out = wx+b $  \n",">\n",">$ \\frac{dout}{dx} = w 　\\blacktriangleright 　 dx = dout ⋅ w $   \n",">$ \\frac{dout}{dw} = x 　\\blacktriangleright 　 dw = dout ⋅ x $   \n",">$ \\frac{dout}{db} = 1 　\\blacktriangleright 　 db = dout $ "],"metadata":{"id":"V7sWbaKG6_sD"}},{"cell_type":"code","source":["def relu_forward(x):\n","    \"\"\"\n","    Computes the forward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - x: Inputs, of any shape\n","\n","    Returns a tuple of:\n","    - out: Output, of the same shape as x\n","    - cache: x\n","    \"\"\"\n","    out = None\n","    ###########################################################################\n","    # TODO: Implement the ReLU forward pass.                                  #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    out = np.maximum(0,x)\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    cache = x\n","    return out, cache"],"metadata":{"id":"NOYzb-hZ8Xrt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<img src=\"note/ReLU.png\" width=\"500px\" height=\"300px\" title=\"ReLU\" />\n","\n","\n","$ out =\n","\\begin{cases}\n","0, & x < 0 \\\\\n","x, & x \\ge 0\n","\\end{cases} $"],"metadata":{"id":"WoW4wHnl9NeI"}},{"cell_type":"code","source":["def relu_backward(dout, cache):\n","    \"\"\"\n","    Computes the backward pass for a layer of rectified linear units (ReLUs).\n","\n","    Input:\n","    - dout: Upstream derivatives, of any shape\n","    - cache: Input x, of same shape as dout\n","\n","    Returns:\n","    - dx: Gradient with respect to x\n","    \"\"\"\n","    dx, x = None, cache\n","    ###########################################################################\n","    # TODO: Implement the ReLU backward pass.                                 #\n","    ###########################################################################\n","    # *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","\n","    mask = np.where(x>0,1,0)\n","    dx = dout*mask\n","\n","    # *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n","    ###########################################################################\n","    #                             END OF YOUR CODE                            #\n","    ###########################################################################\n","    return dx"],"metadata":{"id":"gvfdu3-Yx838"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["$ \\frac{dout}{dx} =\n","\\begin{cases}\n","0, & x < 0 \\\\\n","1, & x \\ge 0\n","\\end{cases} $ \n","\n","　　$\\blacktriangledown $ \n","\n","$ dx =\n","\\begin{cases}\n","0, & x < 0 \\\\\n","dout, & x \\ge 0\n","\\end{cases} $ "],"metadata":{"id":"ujHsmYHR3TG7"}},{"cell_type":"code","source":[""],"metadata":{"id":"VJFTQICS4Xrg"},"execution_count":null,"outputs":[]}]}